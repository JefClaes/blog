<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>infrastructure on Jef Claes</title>
    <link>http://localhost:1313/tags/infrastructure/</link>
    <description>Recent content in infrastructure on Jef Claes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Aug 2018 14:58:00 +0200</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/infrastructure/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GES scavenging and the hidden cost of link events</title>
      <link>http://localhost:1313/2018/08/ges-scavenging-and-hidden-cost-of-link.html</link>
      <pubDate>Sun, 12 Aug 2018 14:58:00 +0200</pubDate>
      
      <guid>http://localhost:1313/2018/08/ges-scavenging-and-hidden-cost-of-link.html</guid>
      <description>Somewhere around a year ago, we started using GES in production as the primary data store of our new loyalty system. The system stores two types of data.
 External services push batches of dumb downed events to the loyalty system. For example: a user logged on, played a game or participated in a competition. These events are transient by nature. Once the loyalty system has processed them, they only need to be kept around for a few days.</description>
    </item>
    
    <item>
      <title>Amazon Redshift - Fundamentals</title>
      <link>http://localhost:1313/2018/05/amazon-redshift-fundamentals.html</link>
      <pubDate>Tue, 01 May 2018 14:17:00 +0200</pubDate>
      
      <guid>http://localhost:1313/2018/05/amazon-redshift-fundamentals.html</guid>
      <description>Late 2017, we set out to replace and upgrade our existing reporting and analytics infrastructure with something that would be a better fit for our workloads. Keeping costs and required maintenance at a minimum would be a nice plus, making for an easy sell. After a bit of research, it was obvious Amazon Redshift had the potential to tick all the right boxes. While steadily porting the most problematic workloads away from our existing infrastructure, I started writing an investigative article on the fundamental concepts of Amazon Redshift.</description>
    </item>
    
    <item>
      <title>Programmatically force create a new LocalDB database</title>
      <link>http://localhost:1313/2014/10/programmatically-force-create-new.html</link>
      <pubDate>Sun, 26 Oct 2014 16:59:00 +0100</pubDate>
      
      <guid>http://localhost:1313/2014/10/programmatically-force-create-new.html</guid>
      <description>I have spent the last week working in an integration test suite that seemed to be taking ages to run its first test. I ran a profiler on the setup, and noticed a few things that were cheap to improve. The first one was how a new LocalDB database was being created.
An empty database file was included into the project. When running the setup, this file would replace the existing test database.</description>
    </item>
    
    <item>
      <title>Databases are growing on me</title>
      <link>http://localhost:1313/2013/12/databases-are-growing-on-me.html</link>
      <pubDate>Sun, 22 Dec 2013 21:53:00 +0100</pubDate>
      
      <guid>http://localhost:1313/2013/12/databases-are-growing-on-me.html</guid>
      <description>I learned all about logical design of relational databases back in school; tables, columns, data types, views, normalization, constraints, primary keys, foreign keys&amp;hellip; At the same time, I learned how to use SQL to put data in, and how to get it out again; INSERT INTO, SELECT, FROM, WHERE, JOIN, GROUP...
In the first project I worked on just out of school, we weren&amp;rsquo;t doing anything interesting with databases; we didn&amp;rsquo;t have that many users, or that much data.</description>
    </item>
    
    <item>
      <title>An event store with optimistic concurrency</title>
      <link>http://localhost:1313/2013/11/an-event-store-with-optimistic.html</link>
      <pubDate>Sun, 10 Nov 2013 18:25:00 +0100</pubDate>
      
      <guid>http://localhost:1313/2013/11/an-event-store-with-optimistic.html</guid>
      <description>Like I mentioned last week - after only five posts on the subject - there still are a great deal of event sourcing nuances left to be discovered.
My current event store implementation only supports a single user. Due to an aggressive file lock, concurrently accessing an aggregate will throw an exception. Can we allow multiple users to write to and read from an event stream? Also, what can we do about users making changes to the same aggregate; can we somehow detect conflicts and avoid changes to be committed?</description>
    </item>
    
    <item>
      <title>Event projections</title>
      <link>http://localhost:1313/2013/10/event-projections.html</link>
      <pubDate>Sun, 27 Oct 2013 17:43:00 +0100</pubDate>
      
      <guid>http://localhost:1313/2013/10/event-projections.html</guid>
      <description>In my first two posts on event sourcing, I implemented an event sourced aggregate from scratch. After being able to have an aggregate record and play events, I looked at persisting them in an event store. Logically, the next question is: how do I query my aggregates, how do I get my state out?
In traditional systems, write and read models are not separated, they are one and the same. Event sourced systems on the other hand have a write model - event streams, and a separate read model.</description>
    </item>
    
    <item>
      <title>An event store</title>
      <link>http://localhost:1313/2013/10/an-event-store.html</link>
      <pubDate>Sun, 20 Oct 2013 17:30:00 +0200</pubDate>
      
      <guid>http://localhost:1313/2013/10/an-event-store.html</guid>
      <description>Last week, I implemented an event sourced aggregate from scratch. There I learned, that there isn&amp;rsquo;t much to a naively implemented event sourced aggregate; it should be able to initialize itself from a stream of events, and it should be able to record all the events it raises.
public interface IEventSourcedAggregate : IAggregate { void Initialize(EventStream eventStream); EventStream RecordedEvents(); } The question I want to answer today is: how do I persist those event sourced aggregates?</description>
    </item>
    
    <item>
      <title>Eventual consistent domain events with RavenDB and IronMQ</title>
      <link>http://localhost:1313/2013/08/eventual-consistent-domain-events-with.html</link>
      <pubDate>Thu, 15 Aug 2013 14:03:00 +0200</pubDate>
      
      <guid>http://localhost:1313/2013/08/eventual-consistent-domain-events-with.html</guid>
      <description>Working on side projects, I often find myself using RavenDB for storage and IronMQ for queueing. I wrote about that last one before here and here.
One project I&amp;rsquo;m working on right now makes use of domain events. As an example, I&amp;rsquo;ll use the usual suspect: the BookingConfirmed event. When a booking has been confirmed, I want to notify my customer by sending him an email.
I want to avoid that persisting a booking fails because an eventhandler throws - the mail server is unavailable.</description>
    </item>
    
  </channel>
</rss>