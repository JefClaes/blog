+++
title = "Bulk SQL projections with F# and type providers"
slug = "2015-10-18-bulk-sql-projections-with-f-and-type-providers"
published = 2015-10-18T15:30:00+02:00
author = "Jef Claes"
tags = [ "CodeSnippets", "DDD", "F#",]
+++
Early Summer, I had to set up an integration with an external partner.
They required of us to daily provide them with a relational dataset
stored in SQL Server. Most, if not all of the data was temporal,
append-only by nature; think logins, financial transactions..  
  
Since the data required largely lived in an eventstore on our end, I
needed fast bulk projections. Having experimented with a few approaches,
I eventually settled on projections in F\# taking advantage of type
providers.  
  
Let's say we have an event for when users watched a video and one for
when users shared a video.

  

We want to take streams from our eventstore and project them to a
specific state; a stream goes in and state comes out.

  

Then we want to take that state, and store it in our SQL Server
database.

  

Some infrastructure that reads a specific stream, runs the projection,
stores the state and checkpoints the projection, could look like this.

  

To avoid data corruption, storing the state and writing the checkpoint
happens in the same transaction.  
  
With this piece of infrastructure in place, we are close to implementing
an example. But before we do that, we first need to install the
[FSharp.Data.SqlClient
package](https://www.nuget.org/packages/FSharp.Data.SqlClient). Using
this package, we can use the [SqlProgrammabilityProvider type
provider](http://fsprojects.github.io/FSharp.Data.SqlClient/) to provide
us with types for each table in our destination database. In the snippet
below, I'll create a typed dataset for the WatchedVideos table and add a
row.

  

I haven't defined this type, nor was it generated by me. The
SqlProgrammabilityProvider type provider gives you these for free, based
on the meta data it can extract from the destination database. This also
means that when you change your table, without changing your code, the
compiler will have no mercy and immediately feed back where you broke
your code. In this usecase, where you rather rebuild your data than
migrate it, the feedback loop of changing your database model becomes so
short, that it allows you to break stuff with much confidence. The only
caveat here is that the compiler must always be able to access that
specific database, compiling without fails. In practice, this means you
need to ship your source with a build script that sets up your database
locally before you do any work.  
  
Going from a stream to a dataset is quite declarative and
straightforward with the help of pattern matching.

  

Storing the result in an efficient fashion is also simple, since the
dataset directly exposes a BulkCopy method.

  

When we put this all together, we end up with this composition.

  

Executing this program, we can see the data was persisted like
expected.  
  

[![](/post/images/thumbnails/2015-10-18-bulk-sql-projections-with-f-and-type-providers-bulkprojection.PNG)](/post/images/2015-10-18-bulk-sql-projections-with-f-and-type-providers-bulkprojection.PNG)

  
In the real world, you also want to take care of batching and logging,
but that isn't too hard to implement.  
  
Having this approach in production for some time now, I'm still quite
happy with how it turned out. The implementation is fast, and the code
is compact and easy to maintain.
